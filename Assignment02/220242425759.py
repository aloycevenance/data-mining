# -*- coding: utf-8 -*-
"""220242425759.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tEzVtw3e9pbLNeZhD7VVJvCZgVj4QTxL

imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

import statsmodels.api as sm

"""load data"""

data = pd.read_excel("Diabetes_Data.xlsx")

print(data.head())
print(data.shape)
print(data.columns)

"""SECTION 1.

question 1.1 correlation matrix
"""

X = data.drop(columns=['Y'])
corr_matrix = X.corr()
print(corr_matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Matrix of Diabetes Variables")
plt.show()

"""QUESTION 1.3 MULTIVARIATE LINEAR REGRESSION"""

X = data.drop(columns=['Y'])
y = data['Y']

"""add constants"""

X_const = sm.add_constant(X)

model = sm.OLS(y, X_const).fit()
print(model.summary())

"""fit regression model and computing MSE"""

y_pred = model.predict(X_const)
mse = mean_squared_error(y, y_pred)
print("Mean Squared Error:", mse)

"""QUESTION 1.5 STEPWISE FORWARD SELECTION

start with empty model
"""

remaining_vars = list(X.columns)
selected_vars = []
current_score = np.inf

"""forwad selection loop"""

while remaining_vars:
    scores_with_candidates = []

    for candidate in remaining_vars:
        temp_vars = selected_vars + [candidate]
        X_temp = sm.add_constant(X[temp_vars])
        model_temp = sm.OLS(y, X_temp).fit()
        mse_temp = mean_squared_error(y, model_temp.predict(X_temp))
        scores_with_candidates.append((mse_temp, candidate))

    scores_with_candidates.sort()
    best_new_score, best_candidate = scores_with_candidates[0]

    if best_new_score < current_score:
        remaining_vars.remove(best_candidate)
        selected_vars.append(best_candidate)
        current_score = best_new_score
    else:
        break

"""final model"""

X_final = sm.add_constant(X[selected_vars])
final_model = sm.OLS(y, X_final).fit()
print(final_model.summary())

"""SECTION 2

LOAD DATA
"""

titanic = pd.read_csv("titanic3.csv")

"""check data

"""

print(titanic.head())
print(titanic.columns)
print(titanic.shape)

"""question 2.2 overall survival probability"""

survival_probability = titanic['survived'].mean()
print("Overall survival probability:", survival_probability)

"""question 2.3 handle missing age and calculate age group and survival table"""

titanic_clean = titanic.dropna(subset=['age'])
titanic_clean['age_group'] = pd.cut(
    titanic_clean['age'],
    bins=[0, 12, 18, 60, 100],
    labels=['Child', 'Teen', 'Adult', 'Senior']
)
survival_table = (
    titanic_clean
    .groupby(['pclass', 'sex', 'age_group'])['survived']
    .mean()
    .reset_index()
)

print(survival_table)

"""question 2.4 logistic regression model"""

model_data = titanic[['survived', 'pclass', 'sex', 'age']].dropna()
model_data['sex'] = model_data['sex'].map({'male': 0, 'female': 1})
X = model_data[['pclass', 'sex', 'age']]
y = model_data['survived']
X_const = sm.add_constant(X)
logit_model = sm.Logit(y, X_const).fit()
print(logit_model.summary())

"""question 2.5 model perfomance"""

y_prob = logit_model.predict(X_const)
y_pred = (y_prob >= 0.5).astype(int)
from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y, y_pred)
accuracy = accuracy_score(y, y_pred)

print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)

"""SECTION 3.

QUESTION 3.3  STOCK DATA $ PCA
"""

import yfinance as yf
from sklearn.decomposition import PCA

stocks = [
    'MMM', 'AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CVX', 'CSCO', 'KO', 'DIS',
    'DOW', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'MCD', 'MRK',
    'MSFT', 'NKE', 'PG', 'CRM', 'TRV', 'UNH', 'VZ', 'V', 'AMZN', 'WMT'
]

stock_data = yf.download(
    stocks,
    start="2020-01-01",
    end="2021-01-01",
    auto_adjust=True
)['Close']

"""compute daily return"""

returns = stock_data.pct_change().dropna()
print(returns.head())
print(returns.shape)

"""corellation matrix"""

corr_matrix = returns.corr()
print(corr_matrix.head())

"""apply PCA"""

from sklearn.decomposition import PCA

pca = PCA()
pca.fit(corr_matrix)

"""extract PCA component"""

components = pca.components_

"""BAR  plot - PC1 WEIGHTS"""

plt.figure(figsize=(12, 5))
plt.bar(stocks, components[0])
plt.xticks(rotation=90)
plt.title("First Principal Component (PC1) Weights")
plt.tight_layout()
plt.show()

"""BAR PLOT PC2 WEIGHTS

"""

plt.figure(figsize=(12, 5))
plt.bar(stocks, components[1])
plt.xticks(rotation=90)
plt.title("Second Principal Component (PC2) Weights")
plt.tight_layout()
plt.show()

"""QUESTION 3.4 SCREE PLOT"""

explained_variance = pca.explained_variance_ratio_
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o')
plt.xlabel("Principal Component")
plt.ylabel("Variance Explained")
plt.title("Scree Plot")
plt.tight_layout()
plt.show()

"""COMMULATIVE VARIANCE"""

cumulative_variance = np.cumsum(explained_variance)

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.axhline(y=0.95)
plt.xlabel("Principal Component")
plt.ylabel("Cumulative Variance Explained")
plt.title("Cumulative Variance Plot")
plt.tight_layout()
plt.show()

"""QUESTION 3.5 PCA SCATTER & DISTANCE ANALYSIS

transform data into PCA scatter
"""

pca_scores = pca.transform(corr_matrix)

"""SCATTER PLOT (PC1 v PC2)"""

plt.figure(figsize=(8, 6))
plt.scatter(pca_scores[:, 0], pca_scores[:, 1])

for i, stock in enumerate(stocks):
    plt.text(pca_scores[i, 0], pca_scores[i, 1], stock)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA Scatter Plot of Stocks")
plt.tight_layout()
plt.show()

"""average point"""

average_point = pca_scores.mean(axis=0)

"""euclidean distances"""

distances = np.linalg.norm(pca_scores - average_point, axis=1)

"""three most stocks"""

distance_df = pd.DataFrame({
    'Stock': stocks,
    'Distance': distances
})

top_3 = distance_df.sort_values(by='Distance', ascending=False).head(3)
print(top_3)